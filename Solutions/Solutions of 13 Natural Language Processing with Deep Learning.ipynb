{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Solutions of 13 Natural Language Processing with Deep Learning.ipynb","provenance":[],"collapsed_sections":["SsiTcxT7jG0n","utulBQ7JuRYQ"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uRTTgx-28Y5E"},"source":["# 13 Natural Language Processing with Deep Learning\n","\n","Welcome to the 13th session of this series on Practical Machine Learning, where we will continue our discussion on Deep Learning. So we've covered a lot on Computer Vision till now, not only from the perspective of Deep Learning, but also in the entire series. In this session, we will be focussing on *Natural Language Processing* (or simply called *NLP* for short), specifically from the perspective of Deep Learning. \n","\n","Before we proceed with NLP, let us discuss what NLP even is, and what it implies in today's world. \n","\n","Till now, we have understood some basics about Natural Language Processing (in Session 4, on Bayesian Learning). But, what does it mean in the first place?\n","\n","Natural Language Processing is broad field in Artificial Intelligence, that deals with understanding language. This includes building models that understand speech, audio, text, etc. For example, Google Translate, an AI Model, somehow knows how to convert a peice of text/speech into another language. You say \"Hi Siri\" or \"Hey Google\" or \"Alexa\", and your home/phone assistant responds to you in an intelligent way. And this is where today's state of NLP lies. But there's so much that is yet to be developed and discovered. Let us talk about the aspirations of this feild. \n","\n","NLP is not only about identifying language, but also understanding it. Currently NLP has not quite reached there. For example, we still don't know how to build models that actually *understand* the semantics of a language. Like, just apart from identifying English, we don't know how to build models that understand sarcasm, or emotions, or slightly misspelt/mistyped words. Today's models don't understand context well. Infact, there are models now that can generate entire segments of texts. On a superficial inspection, it *seems* that the model generates sensible text, but on closer inspection, you would find out that most of the text, in most cases doesn't make great sense. That is why, even in 2021, most AI chatbots don't work well. \n","\n","To be fair, it is not a fault in the modeling techniques used in NLP, but more so because of our (humans') own lack of understanding of how to model language on a very broad level. Though, I expect, that in a few years, we will be able to develop much better modeling methods to understand language. With that, there will also be new types of models that will be more suitable for NLP tasks. Present Models and optimizers tasks just do the job, but it is possible that NLP techniques will see some major changes in the coming few years, from the very fundamentals, including optimization techniques. The very basis of communication in human society is language, and unless we figure out how AI effectively processes language, it is hard for it to make its place in soceity. \n","\n","But we shouldn't deny that NLP has made some great progress over the few years, and Deep Learning is the reason behind it. If you remember building a text generator in session 4, you will see, how using Deep Learning, we can significantly improve the model. In this session, we will be learning how to build state of the art language models, that generate text, and are also capable of classifying texts into categories. We will be learning how to build Deep Learnign models that are capable of handling texts (called as *Recurrent Neural Networks* (*RNNs*), and also about some variants that have been known to perform effectively, including *LSTMs (Long Short Term Memory Models)* and *Transformers*. \n","\n","Let us begin this session by installing the necessary libraries. We'll be using the fastai library to handle data pipelining and training, so that we can really focus on building our models. And under the hood, we'll essentially be using the PyTorch library, and when you import the fastai library, you're automatically importing all necessary functionalities from the PyTorch library.\n"]},{"cell_type":"code","metadata":{"id":"pINuoLJHPKJW"},"source":["!pip install sentencepiece >./temp\n","!pip install --upgrade fastai >./temp\n","from fastai.text.all import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snumdWWwLPwJ"},"source":["## Setting up the Data\n","\n","Before starting to explore Language Models, we need a language(text) dataset, on which we perform our experiments. We'll be using the [IMDb Movie Reviews](https://ai.stanford.edu/~amaas/data/sentiment/) by Stanford University, that contains movie reviews from IMDb's website. It contains 50,000 labeled datapoints, the labels of which are a binary class label depicting the sentiment of the review (whether the review is a 'positive' or a 'negative' review), and an additional 50,000 unlabeled movie reviews. You might have guessed that (atleast) one of the things that we will build will be a text classifier using this data. But that's not all! We will discuss how to use these unlabeled datapoints as well. We will go much beyond a text classifier, and essentially build a model that understands language in general!\n","\n","Fastai gives cloud access to download famous datasets, this being one of them. The cloud link can be found by typing `URLs.IMDB`. We can then download the dataset by using the `untar_data` function, which not only downloads the dataset, but also uncompresses the data. Datasets can be in the form of zip files, tar files, 7z files, gzip files, etc.,  but `untar_data` automatically handles all these internally. And finally the function returns the local path where the dataset is stored. "]},{"cell_type":"code","metadata":{"id":"NH6xPDjXjGO5"},"source":["path=untar_data(URLs.IMDB)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1rYHKa8RAyP"},"source":["And then we can simply see what is present in the path, by using a custom method of the Path class, `ls`, that enumerates all items in a particular path location. "]},{"cell_type":"code","metadata":{"id":"w_44frtZQ-f3"},"source":["path.ls()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ayb4d9M-RN_i"},"source":["The text files are present in three folders, namely the `train`, the `test` and the `unsup` (for unsupervised, or unlabeled) folder. So before we start talking about any distinction between all these folders, let us create a list of *all* text files. Essentially we are also throwing away all labels for now, and just collecting the raw text files. We will discuss why in the following section. So before we start talking about any distinction between all these folders, let us create a list of *all* text files. Essentially we are also throwing away all labels for now, and just collecting the raw text files."]},{"cell_type":"code","metadata":{"id":"r0Farvrgfhob"},"source":["files=get_text_files(path,folders=['train','test','unsup'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNXk1LhBSUbL"},"source":["Just for visualization, let us see what these reviews look like."]},{"cell_type":"code","metadata":{"id":"UxxcLv5bhNPb"},"source":["txt=files[0].open().read()\n","txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D793hhfpSuW3"},"source":["Using these, we will be discussing about some preprocessing techniques that are essential to understand before we even jump into building models. Why do we need preprocessing? Obviously because models don't understand words. They only understand numbers. So one logical solution would be to map each word in our entire *corpus* (meaning, our collection of documents/text files) to a unique number, and simply replacing each word with its corresponding number, and using that to build our models.\n","\n","There are 2 steps involved in this whole process. First we need to identify different words in our text files, and then break our sentences into lists of these indivdual words, or components. Once we do this, we don't treat our data as strings containing sentences and paragraphs, but as sequences of small chunks (words). These small chunks are called *Tokens*, and this whole process is called *Tokenization*. \n","\n","And secondly, create a mapping between these words and numbers. But there are a few nuiances in these processes that need to be discussed. \n","\n","So, let us start by creating Tokens of our text. "]},{"cell_type":"markdown","metadata":{"id":"XVOB3uIRYTpP"},"source":["### Tokenizing our text\n","\n","By default, fastai uses a library called [*SpaCy*](https://spacy.io/api/tokenizer) that handles the tokenization process. You might think that the process is as simple as simply separating words between spaces or punctuation marks. But in practice, its not as simple as it seems. Here are a few examples of the problems we may face. \n","\n","* How do we deal with a word like “don’t”? Is it one word or two? \n","\n","* What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? \n","\n","* What about languages like German and Polish, which can create really long words from many, many pieces? \n","\n","\n","These are just a few examples of the problems that we may face. \n","\n","#### Word Tokenization\n","\n","Spacy has a sophisticated API to create word tokens, that can handle special English Words (like don't is separated as do and n't, and treating the `.` between two words as a full stop, but not in special words such as 'U.S.'), URLs, etc. This API can be easily accessed by a fastai class called `WordTokenizer`. "]},{"cell_type":"code","metadata":{"id":"tj7PlHRMj-_a"},"source":["spacy=WordTokenizer() #we store the class initialization in a variable called spacy\n","toks=first(spacy([txt])) # spacy[text] will create a generator that has been applied on all strings. You can then use the first function to get a list of all these tokens\n","toks #display the tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSBE3gCJeoAX"},"source":["You can see below, that SpaCy knows how to handle full stops and dots in between special abbreviations such as in 'U.S.', and also words like don't properly.  "]},{"cell_type":"code","metadata":{"id":"w-xwMcmTkQi3"},"source":["print(first(spacy([\"The U.S. dollar $1 is 75.00 rupees today. I don't think you should convert your rupees right now\" ]))) # the full stop is a separate token, but U.S. is a single token "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNcofslefNfM"},"source":["\n","But we will be using fastai's `Tokenizer` class, which builds on top of the `WordTokenizer` class. It contains some additional rules like adding tokens that represent the begenning of the stream, or end of the stream, and replacing meaningless repetitions of characters, denoting capital letters by a separate token and converting the word to lowercase, etc). \n","\n","This is because we would not only like to remove redundant information, but also like to provide our text models with some additional information, like, when the stream ends (which may indicate to the model to forget whatever it has learnt till now), or when the stream begins (which may indicate to the model to start afresh), or indicate where a capitalized word is present, or replace unknown (or very rare) words with a special token (like `xxunk`)"]},{"cell_type":"code","metadata":{"id":"GpCEbAK1lGZS"},"source":["tkn=Tokenizer(spacy)\n","tkn(txt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3glktp77gF1A"},"source":["You can see all the rules that fastai applies here. You can find what each one of these does, through the [documentation's page](https://docs.fast.ai/text.core.html#Preprocessing-rules). "]},{"cell_type":"code","metadata":{"id":"TtuqGjall1dP"},"source":["defaults.text_proc_rules"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj2L4LJ3ghHJ"},"source":["or for those who like to be more hands on, you can dive right into the source code like so..."]},{"cell_type":"code","metadata":{"id":"pcL1GfsdkKiy"},"source":["??core.replace_all_caps "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dstoGx_CA3LS"},"source":["So what we've done is essentially built a Word Tokenizer. But now let us talk about a different kind of problem. \n","\n","#### Subword Tokenization\n"," \n","What if we're dealing with languages like Japanese and Chinese, that don’t use bases at all, and don’t really have a well-defined idea of word?\n","\n","In that case, it may be a better idea to not tokenize words, but commonly occuring parts of words.The problem is, how do we make a function that can identify parts of a word? We can easily make a function to identify spaces, or in another extreme, a function that separates each character, but what rues do we use to group a few characters. One way is is to group the most commonly occuring sequence of characters. This is a lengthy process, because you need to go through the entire document multiple times in order to count the frequency of characters. \n","\n","Now we obviously need to specify some limit to the number of tokens in our vocabulary, because in a case where there are no limits, the function would ultimately tokenize to the very end, and would end up tokenizing even single characters that don't occur frequently enough with other characters, in which case, our final vocabulary can potentially be infinitely long. So in essence, it is important to set some limit to the vocaubulary size, containing the most commonly occuring subwords. All other words will be represented as unknown (`xxunk`) tokens. "]},{"cell_type":"code","metadata":{"id":"BnpWMWhIk88j"},"source":["txts=L(o.open().read() for o in files[:2000]) #getting 2000 characters "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kF9NhIdKpKuf"},"source":["We define a function called as `subword` that counts up the frequencies of commonly occuring subwords. The exact working is quite complex, but feel free to go into the source code of the `SubWordTokenizer` class. "]},{"cell_type":"code","metadata":{"id":"aNR5XhDGJRFS"},"source":["def subword(sz): #sz is the maximum number of tokens that will be created \n","    sp=SubwordTokenizer(vocab_sz=sz)\n","    sp.setup(txts) #this creates tokens\n","    return ' '.join(first(sp([txt]))[:40])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmefIx6KKzgY"},"source":["subword(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRykZY4O12e8"},"source":["Here we use a vocabuary limit of 1000 tokens. The underscores represent spaces that occur in the text. This is to differentiate the spaces that occur between tokens of subwords and the spaces that naturally occur in the text documents. \n","\n","Just for comparison, let us compare the tokens that are generated when we change the vocab limit to a shorter length. "]},{"cell_type":"code","metadata":{"id":"Vmmfg7gyK6V-"},"source":["subword(200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRiABy7g36KX"},"source":["You may notice that the tokens in the latter case are generally smaller than the former. That's because when we increase the vocab length, we are also accounting for lesser and lesser frequent substrings, of which whole words are more likely to be a part of. When we limit our vocabulary to a lesser limit, smaller subwords (like \"on\" or \"ed\" ) are more likely to be more frequent that whole words. \n","\n","Just for a better understanding, we will use an even larger vocabulary size, and you will notice, that it is almost the same as counting entire words as tokens."]},{"cell_type":"code","metadata":{"id":"n2Xq7aBwMpeh"},"source":["subword(10000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aB_wahId48O5"},"source":["Once we've setup our tokens, we need to *numericalize* them, ie, convert each token to a unique representative number, because machines can only understand numbers. For the rest of the session, we will be using word tokens, rather than subword tokens. "]},{"cell_type":"markdown","metadata":{"id":"BaepWBwdOmrN"},"source":["### Numericalizing the tokens\n","\n","For simplicity, let us create tokens on a smaller corpus, and convert it to tokens using the word tokenizer provided by fastai.  "]},{"cell_type":"code","metadata":{"id":"huPUsQywPkmU"},"source":["toks200 = txts[:200].map(tkn)\n","toks200[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U2XBlm217B7J"},"source":["To map each token to a unique number, we can use fastai's `Numericalize` class. This class also provides us with an attribute `vocab` to map the numericalized tokens back to their original values (words). "]},{"cell_type":"code","metadata":{"id":"JRxcV4TePqz2"},"source":["num=Numericalize()\n","num.setup(toks200) #the setup function does the mapping"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2kTk_-j7eEd"},"source":["let us see what what the numericalized tokens would look like. the `num` class takes a list of strings, and using the tokens generated on the corpus (`toks200`), and replace each word with its corresponding number. Any word that wasn't there in the vocabulary will be replaced with an unknown token (`xxunk`)."]},{"cell_type":"code","metadata":{"id":"inDXp3L0QlB9"},"source":["nums=num(toks)\n","nums"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VIY5DEf78Z66"},"source":["We can even map back these tokens to the original text using the `vocab` attribute of the Numericalize class."]},{"cell_type":"code","metadata":{"id":"Ajsox4WiR8eE"},"source":["' '.join(num.vocab[o] for o in nums) #notice the frequent xxunk's. That is because of the words that were present in toks, but not in the corpus (toks200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0uvid02v9eUh"},"source":["## Creating our Language Model\n","\n","We have understood the key ideas behind preprocessing our text. Now we need to feed the data into the model and create our text classifier. \n","\n","The fundamental argument is as follows. We know that transfer learning generally helps improve model performance. In the case of images, we used a pretrained model trained on ImageNet or some other dataset, the reason being that we know that training on a larger dataset helps the model understand basic image features that we would expect from the model over any standard image classification problem, like, features of humans, or trees in general. \n","\n","The NLP equivalent of that would be to use weights from a model that already knows the innards of the English Language, like, for example, basic grammar, sentence construction, the meanings of punctuation marks, contexts and so on. This is called a *Language Model*, ie a model that - understand language in a very general sense. The pretrained weights can be used to fine tune our classifier of text. \n","\n","In the case of images, we pretrain our model on a supervised learning dataset, ie a dataset that itself brings forth a supervised learning problem (classification). Turns out that you don't need explicit labels to create language models that understand language. You can train the model without any explicit labels. Essentially we're training the language model using a *self supervised learning* approach. Self Supervised Learning is a type of learning where the task is to learn the general features of the data, rather than doing a mapping from some input to some output. Let us discuss how this is done. \n","\n","So, one of the most common datasets used for pretraining language models is the [WikiText103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), which is a dataset containing lots of text from the WikiPedia website, from random articles (over 100 million tokens in total). The idea behind this dataset is, that, no matter what your final task is, if it involves English, your model would need to understand basic grammatical, syntactical and logical rules involved in the language. Now, even though Wikipedia may contain a more formal language than the task you wish to do, but it is safe to assume that there is still a lot to learn about basic rules of English from Wikipedia, and this will help our final model, no matter what. So our task is to get a model that has been pretrained on a dataset like WikiText103. \n","\n","But how is this model even trained in the first place? As we mentioned, we use a supervised learning technique. Obviously the neural net that was used to train this model uses some sort of input/output mapping (because that is how neural nets work, there is no other way). But then, how do we make Neural Nets work without explicitly mentioning an output? Well this is done by making the output the same as the input (or of the same type as the input). In which case, the Neural Net would essentially be doing a mapping from the input to the input itself. This is another broad field in Machine Learning, and is known as Representational Learning. \n","\n","In our specific context, we feed a series of tokens as our input, and the target output is the same text, just offset by one token. So our model would be trying to predict the next sequence of text, whenever a peice of text is given. This is how supervised learning approach works in the context of NLP. \n","\n","This is how this would look like. This type of mapping of a text to a peice of text that is offset by only one token is automatically done by the `LMDataLoader` class. It is essentially a child class of the DataLoader class, the job of which is to break the data into batches and feed it into the model.\n","\n","One more important thing to note is that, usually in the case of Images, the dataloader also shuffled the data, so that the model doesnt learn the sequence of images. But here, we can't shuffle text tokens, or they won't sense anymore. `LMDataLoader` takes care of that for us. "]},{"cell_type":"code","metadata":{"id":"T71AO2wOTsiC"},"source":["nums200=toks200.map(num)\n","dl=LMDataLoader(nums200)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRj3lO9lZGlW"},"source":["x,y=first(dl)\n","x.shape,y.shape #each batch has 64 items, each containing a fixed number of tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wC6vzvYJvHt"},"source":["You can see that x and y are offset by only one token. Our Neural Net only needs to learn how to take in a sequence of text, and predict the next sequence of words."]},{"cell_type":"code","metadata":{"id":"uSqocTgTaT4i"},"source":["x[0],y[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ZIWap13KBDa"},"source":["Let us also map these tokens back to words, because we can't visualize numbers as well as words themselves. "]},{"cell_type":"code","metadata":{"id":"3FyXi_fZZK2Z"},"source":["' '.join(num.vocab[o] for o in x[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFYb_DweJ7KJ"},"source":["' '.join(num.vocab[o] for o in y[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdb5SaJ5IDOX"},"source":["\n","Now once you have some pretrained weights, you can use those in the final task, which in this case is text classification (classification of IMDb reviews into positive and negative reviews). All you have to do now is change the task. Now instead of the target output being a sequence of text, it will be classes. Sounds straightforward!\n","\n","Let us go one step beyond.\n","\n","We will be discussing an interesting Language Modeling approach called *ULMFiT* (Universal Language Model Fine-tuning), which was introduced in [this](https://arxiv.org/abs/1801.06146) paper. Let us discuss the key ideas from this paper through our example of creating an IMDb text classifier. \n","\n","![](https://drive.google.com/uc?id=1itzIngHaDIyVWhyBz2oEia8tN_OsegvZ)\n","\n","After creating the WikiText Language Model, that understands the English Language, our problem is this. Our model has also sort of overfitted on the Wikipedia text. So now, its style of language is more formal, and its more likely to predict text that you would see in WikiPedia, rather than movie review style text. This paper introduced a clever solution to that. \n","\n","As an extension to the self-supervised pretraining stage, why not *fine-tune* the language model on our specific dataset, so that the language model learns the style of text that is more common in the target dataset (IMDb, in this case), and carry on the classification text from that point onwards. Let us see how to do that!\n"]},{"cell_type":"markdown","metadata":{"id":"SsiTcxT7jG0n"},"source":["### Finetuning a Language Model\n","\n","just for the purpose of language modeling, we don't need any explicit labels. So we gather *all* the data that we can find, whether its is from the training set, or the validation set, or the extra unlabeled data that was present in the IMDb dataset. \n","\n","Now, you may be wondering, how come we're training the model on the validation set. That would be true if we were performing the classification task, but here, we are creating a language model ,which does not involve any specific labels. Meaning, we don't evaluate the performance of the model on a separate sub-dataset. In this case, it is wise to simply pool all the data you can find and build the language model on top of that. "]},{"cell_type":"code","metadata":{"id":"AUB82YPmZOpO"},"source":["get_imdb=partial(get_text_files,folders=['train','test','unsup'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7o4XCmKUdit_"},"source":["dls_lm = DataBlock(\n","    blocks=TextBlock.from_folder(path,is_lm=True), #is_lm tells that explicit labels are not required, and the target would simply be text offset by one token\n","    get_items=get_imdb, splitter = RandomSplitter(0.1)\n",").dataloaders(path,path=path,bs=128,seq_len=80) #each batch would be of length 80 tokens, and a total of 128 batches are required"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZ_UMTYRPgeP"},"source":["We can see how the data would look like, along with the target texts"]},{"cell_type":"code","metadata":{"id":"mGZvHZmBeMtL"},"source":["dls_lm.show_batch(max_n=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TeuZ91YGjj60"},"source":["After which we can simply train this model as before, using the Learner class. We will be using an advanced architecture called *AWD-LSTM* (which we will be implementing later on), and this dataset has already been pretrained on WIKITEXT103. \n","\n","We use a new metric here called Perplexity, which is often used in NLP task. It is nothing but the exponential of the cross entropy loss (`torch.exp(cross_entropy)`), and depicts how well the model has been able to predict the next sequence of tokens. \n"]},{"cell_type":"code","metadata":{"id":"ctswTRZjis3Z"},"source":["learn=language_model_learner(\n","    dls_lm,arch=AWD_LSTM,drop_mult=0.3,\n","    metrics=[accuracy,Perplexity()]).to_fp16()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mM3ZWusTRGLK"},"source":["Before we begin fine tuning our model, let us see how it performs using the pretrained WIKITEXT weights. We would expect it not to be like Movie reviews, but a more formal language. "]},{"cell_type":"code","metadata":{"id":"18JAoxX2KjC-"},"source":["TEXT=\" I liked this movie because\"\n","N_WORDS=40\n","N_SENTENCES=2\n","preds=[learn.predict(TEXT,N_WORDS,temperature=0.75) for _ in range(N_SENTENCES)]\n","print(\"\\n\".join(preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-6cRTrgRu_2"},"source":["Now let us perform the fine tuning task. Its a slighlty lengthy process, just because of the sheer size of the data, so you can run the next few cells, and leave you PC, have lunch, or take a walk, and come back. "]},{"cell_type":"code","metadata":{"id":"Ec1OCbNsm9eg"},"source":["learn.fit_one_cycle(1,2e-2) #As we've seen before, fastai trains the model in two stages. The first stage involves finetuning only the final few layers, that have beeem randomly initialized"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"snSD-jmho2b8"},"source":["learn.save('1epoch') #saving the model state to a file\n","learn.load('1epoch')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"668SKLkbqjlL"},"source":["learn.unfreeze() #making the rest of the model trainable, by setting their `requires_grad` as True (since only then can their parameters be updated)\n","learn.fit_one_cycle(10,2e-3) #this is the second stage of training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFVmy_sJVhVA"},"source":["path_mdl=Path('.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUcYa5tXT3sx"},"source":["You can optionally save it to your Google Drive to access it later. If you wish to do this, simply run the following lines of code, and change the path to your desired location on your google drive. "]},{"cell_type":"code","metadata":{"id":"-P7UhSIoijkb"},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# path_mdl=Path('/content/gdrive/My Drive/') # modify this to change the location"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fP6e3ty4Vkm7"},"source":["learn.save_encoder(path_mdl/'finetuned_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RUG9jcnrCwu"},"source":["before building the *classifier*, lets see how this is performing anyways..."]},{"cell_type":"code","metadata":{"id":"inqrAuJqlFVU"},"source":["TEXT=\" I liked this movie because\"\n","N_WORDS=40\n","N_SENTENCES=2\n","preds=[learn.predict(TEXT,N_WORDS,temperature=0.75) for _ in range(N_SENTENCES)]\n","print(\"\\n\".join(preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f57o8KCUU2gu"},"source":["You can see how well this performs in comparison to the Text generation model we created in Session 4, on Bayesian Learning. Not only is the grammar and syntax much better, but it simply makes so much more sense now, and you have thus created a state of the art text generator model with only a couple of hours of training. \n","\n","The next step is to build our text classifier. "]},{"cell_type":"markdown","metadata":{"id":"utulBQ7JuRYQ"},"source":["## Text classifier\n","\n","Now, we need to reinitialize the dataloaders, because we need to tell it that now, our target is not to generate a sequence of tokens, but a category. "]},{"cell_type":"code","metadata":{"id":"SXqvB45PrYP8"},"source":["dls_classification = DataBlock(\n","    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock), #we don't pass is_lm=True to TextBlock.from_folder\n","    #we also pass in the vocab here, because otherwise the vocab generated here may not match the vocab used in the fintuning task, which may lead to results which makes no sense\n","    get_y=parent_label,\n","    get_items=partial(get_text_files,folders=['train','test']), #now we dont used the unsupervised text anymore\n","    splitter=GrandparentSplitter(valid_name='test') \n",").dataloaders(path,path=path,bs=128,seq_len=72)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lB2aHDFyYjw-"},"source":["And let's see what our data will look like now, along with the targets."]},{"cell_type":"code","metadata":{"id":"fbeMydtqu5qk"},"source":["dls_classification.show_batch(max_n=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DemYdOtGYv5f"},"source":["Now that we've changed the dataloaders, we can train the model in the same way. We do need to import the pretrained network weights, which will help us identify *features* of the language. So essentially, the pretrained model will act as the encoder. We can load the weights into a new learner using the `load_encoder` method of the learner class. "]},{"cell_type":"code","metadata":{"id":"e9lBSF-qTzVE"},"source":["learn=text_classifier_learner(dls_classification,AWD_LSTM,drop_mult=0.5, metrics=accuracy).to_fp16()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sU0jeM9EVaSE"},"source":["learn = learn.load_encoder(path_mdl/'finetuned_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VuUriB70ZYDZ"},"source":["And let us finally train the model. "]},{"cell_type":"code","metadata":{"id":"xywlrfgVVxNC"},"source":["learn.fit_one_cycle(1,2e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11y-Ndx1Vzud"},"source":["learn.freeze_to(-2)\n","learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKp-pWioWF1b"},"source":["learn.freeze_to(-3)\n","learn.fit_one_cycle(2,slice(5e-3/(2.6**4),5e-3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxrtNoKSWwIk"},"source":["learn.unfreeze()\n","learn.fit_one_cycle(2,slice(1e-3/(2.6**4),1e-3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lFtt-QHZbnw"},"source":["That's a wonderful accuracy. This is almost close to the best accuracy ever achieved on this dataset. The best accuracy is about 96%, and involves some very complex data augmentation techniques, including translating the text to another language, and then translating it back to the original language. But let us not go into such complicated endeavors for now. This accuracy is wonderful in itself!\n","\n","\n","Now having learnt how to build these models, let us build these models from scratch. We'll be starting with a simple Neural Net that can handle sequence of texts, called *RNNs*, or Recurrent Neural Networks. From there on, we will go on to build more advanced models. "]},{"cell_type":"markdown","metadata":{"id":"RSxeLvKUjUk_"},"source":["## Creating RNNs from scratch\n","\n","Before we create an RNN, let us first set up some basic type of data. The IMDb dataset is too large for experiments, and as you may have seen, even simple models take many hours to train because of the size of the dataset. So for simplicity, we choose a dataset that simply includes the first 10,000 numbers written sequentially in word form. "]},{"cell_type":"markdown","metadata":{"id":"9v5QI-4HjUbf"},"source":["### Setting up a very basic dataset "]},{"cell_type":"code","metadata":{"id":"xaOLErsEXFkh"},"source":["path=untar_data(URLs.HUMAN_NUMBERS)\n","path.ls()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQKbbyBxj3uc"},"source":["#initially lets just join both the training and the validation dataset\n","lines=L() #empty list\n","with open(path/'train.txt') as f: lines+=L(*f.readlines())\n","with open(path/'valid.txt') as f: lines+=L(*f.readlines())\n","lines"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OPYyjbfkc5O5"},"source":["Instead of spaces and new line characters, let us just replace all separaters by one common separateor, which is a ' . '. "]},{"cell_type":"code","metadata":{"id":"HBgtiF9VkRN4"},"source":["text = ' . '.join([l.strip() for l in lines]) #the strip function removes all separators including spaces, newlines, tabs, etc\n","text[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFIB3CPIky_A"},"source":["#for demonstration purpose\n","'  asd   \\n \\t'.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qe1UpfSqdqIP"},"source":["And let us now split this text into tokens. Note that the full stop is meant to be part of the sequence as it tells the model when one number finishes, and the next starts. "]},{"cell_type":"code","metadata":{"id":"Lze-d1Euk2Hl"},"source":["tokens=text.split(' ')\n","tokens[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yic8mQJLeAM_"},"source":["And since this text contains many repetitions of words, we will first create a vocabulary of unique words. "]},{"cell_type":"code","metadata":{"id":"ImxxLnaSmde1"},"source":["#to numericalize, we need to create a list of all unique words (aka vocab)\n","vocab=L(*tokens).unique()\n","vocab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HM3QnF66eHh3"},"source":["So this tells us, that in the first 10000 numbers written in words, there are only 29 unique words, plus one for the full stop. "]},{"cell_type":"code","metadata":{"id":"xRAC0NJvmpP9"},"source":["print(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1Lz6fBneTlp"},"source":["And now we need to create a mapping from words to numbers. We store these mappings in a dictionary, called `word2idx`. "]},{"cell_type":"code","metadata":{"id":"mnn_wcY-oR5V"},"source":["word2idx={w:i for i,w in enumerate(vocab)}\n","nums=L(word2idx[i] for i in tokens)\n","nums"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_25aJHqnVGE"},"source":["### Building a simple RNN\n","\n","Just for experimental and comparison purpose, let us build a model that resembles the model we built in session 4. If you remember, the exercise involved building a model that predicted the next word given 2 previous words. In this case, we will be building a model that predicts the word given 3 previous words. For that, all we need to do is to modify the dataset in which the input x contains a tensor of 3 word indexes, and the output y contains the index for the next word. "]},{"cell_type":"code","metadata":{"id":"XcWJhY56m3Tl"},"source":["# creating a simple Neural Net that predicts next word on the basis of the previous three words. Let us set up the dataset for that purpose\n","\n","#right now lets just test it out on tokens, to see what it will look like in the end\n","L((tokens[i:i+3],tokens[i+3]) for i in range(0,len(tokens)-4,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JCQkQmMo2ux"},"source":["#now let us do it for real. Remember we need tensors for PyTorch. Also tensors will take the numericalized tokens\n","seqs=L((tensor(nums[i:i+3]),nums[i+3]) for i in range(0,len(nums)-4,3))\n","seqs \n","#its just a classification problem now, with input as tensor of shape 3,1 and output as a single number ranging from 0 to len(nums)-1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DwaKIFXfSk8"},"source":["Once we set up the dataset, we need to put them in DataLoaders. "]},{"cell_type":"code","metadata":{"id":"TnNqeCBOpY-o"},"source":["#feed these into a Dataloader.\n","bs=64\n","cut=int(len(seqs)*0.8)\n","dls=DataLoaders.from_dsets(seqs[:cut],seqs[cut:],bs=64,shuffle=False) #in language models, we shouldnt shuffle the sequence of words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75B3sF-Rfac2"},"source":["Let us understand how an RNN is built. An RNN is essentially a looping Neural Network. It takes in one input at a time, and passes it through a Linear Layer to get activations. These activations are added to the next word in the sequence, and the result is again passed through the Linear Layer to get new activations. Here we have three words as input, so after repeating this process for a total of three times, we will get some activations from the Linear Layers, which can be passed to a final linear layer (that outputs some probability about which token is the final prediction). This is what an RNN is.\n","\n","![](https://drive.google.com/uc?id=1SprFLnlXXIvTarkaR5XSGrMShrUNdCLe)\n","\n","In comparison to a standard fully connected Neural Network, where we had an input layer, multiple hidden layers, and one final output layer, an RNN is simply the input layer, the output layer, and one single hidden layer, the output of which is fed into itself again and again. Hence the name *Recurrent* Neural Network. \n","\n","There is one small tweak in this. \n","\n","Instead of directly feeding the tokens as the input, what if we had the ability to represent each token as a much more sophisticated tensor of features? We call this an Embedding Matrix, which basically maps each token to a new tensor containing different features. And these features can be learned through standard optimization techniques. \n","\n","\n","![](https://drive.google.com/uc?id=1U6EXZ8uFikW-i12qet9sIi5dn37GeWS2)\n","\n","This basically helps us represent more information in comparison to one single token number. So this is essentially a mapping from a token number to an entire tensor of learnable parameters."]},{"cell_type":"code","metadata":{"id":"jKEph4W7x-VU"},"source":["#refactoring the first LM model\n","class LMModel1(Module):\n","    def __init__(self,vocab_sz,n_hidden):\n","        self.i_h=nn.Embedding(vocab_sz,n_hidden) #embedding layer\n","        self.h_h=nn.Linear(n_hidden,n_hidden) #linear layer: to create activations for the next word\n","        self.h_o=nn.Linear(n_hidden,vocab_sz) #final layer to predict the fourth word in the end\n","\n","    def forward(self,x):\n","        h=0\n","        for i in range(3):\n","            h=h+self.i_h(x[:,i])\n","            h=F.relu(self.h_h(h))\n","        return self.h_o(h)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdEwQHE7iynZ"},"source":["Let us train this model using our Learner class, and see how it performs."]},{"cell_type":"code","metadata":{"id":"LDFyKtwLtlgu"},"source":["learn=Learner(dls,LMModel1(len(vocab),64),loss_func=F.cross_entropy,metrics=accuracy)\n","learn.fit_one_cycle(3,1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5mnVfXbi8ws"},"source":["This may look like a terrible performance, but let us compare this with a model that would make completely random predictions. "]},{"cell_type":"code","metadata":{"id":"uJpa8LiXw-jo"},"source":["1/len(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bvrwq2a6jDb7"},"source":["So you will realize that the model has essentially learnt atleast something. \n","\n","Now let us compare this with an even more sophisticated dummy model, that only predicts THE most commonly occuring word. "]},{"cell_type":"code","metadata":{"id":"zUJ87Futt45v"},"source":["#checking if this model is any good at all?\n","#comparing with a model that would only predict the most commonly occuring word \n","n,counts=0,torch.zeros(len(vocab))\n","for x,y in dls.valid:\n","    n+=y.shape[0]\n","    for i in range_of(vocab):counts[i]+=(y==i).long().sum()\n","idx=torch.argmax(counts)\n","idx,vocab[idx.item()],counts[idx].item()/n\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k99HXIVxyB0X"},"source":["thousand is the most common word. And if we only predicted the word thousand, we would get an accuracy of 15% only, so the RNN is actually working okayish. "]},{"cell_type":"markdown","metadata":{"id":"oZujfeyJMlEc"},"source":["### Improving the RNN\n","\n","Let us first do a very simple tweak to this model, that will help us save a lot of GPU memory, which is based on the problem, that PyTorch keeps track of all computations since the initialization of any layer. So once you've propagated through the three layers, you essentially don't need the computational history from previous iterations. So we can simply *detach* the tensor from the computation history, and it will no longer occupy space in the GPU. "]},{"cell_type":"code","metadata":{"id":"I8PJ8tweLwcN"},"source":["class LMModel2(Module):\n","    def __init__(self,vocab_sz,n_hidden):\n","        self.i_h=nn.Embedding(vocab_sz,n_hidden) #embedding layer\n","        self.h_h=nn.Linear(n_hidden,n_hidden) #linear layer: to create activations for the next word\n","        self.h_o=nn.Linear(n_hidden,vocab_sz) #final layer to predict the fourth word in the end\n","        self.h=0\n","\n","    def forward(self,x):\n","        for i in range(3):\n","            self.h = self.h + self.i_h(x[:,i])\n","            self.h=F.relu(self.h_h(self.h))\n","        out =self.h_o(self.h)\n","        self.h=self.h.detach()\n","        return out\n","    \n","    def reset(self): self.h =0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Xw-TI1SmIc_"},"source":["To use this model, we need to make sure that all batches are sequentially ordered. So let us build this functionality for our purpose. \n"]},{"cell_type":"code","metadata":{"id":"sU6gQMivXgNO"},"source":["bs=64\n","m=len(seqs)//bs\n","m,bs,len(seqs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spHTrXDcc674"},"source":[" def group_chunks(ds,bs):\n","     m=len(ds)//bs\n","     new_ds=L()\n","     for i in range(m): new_ds+=L(ds[i + m*j] for j in range(bs))\n","     return new_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g56_klc8d_OR"},"source":["cut = int(len(seqs)*0.8)\n","dls=DataLoaders.from_dsets(\n","    group_chunks(seqs[:cut],bs),\n","    group_chunks(seqs[cut:],bs),\n","    bs=bs,drop_last=True,shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8sAazk6ms6Y"},"source":["Let train our model using this little tweak. You will also notice a callback (`cbs`) named ModelResetter. Don't worry about it. A callback is essentially a function called at a certian stage of training (like after an epoch ends, or after loss is calucated, or after parameters are updated, and so on). In this case, ModelResetter calls the models `reset` method before the training/validation cycle begins, and also at the very end of training. This helps the model start afresh.  Don't worry about the details for now. "]},{"cell_type":"code","metadata":{"id":"x3DIsZkUpW5W"},"source":["??ModelResetter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgFLGfBxfG9z"},"source":["learn=Learner(dls,LMModel2(len(vocab),64),loss_func=F.cross_entropy,metrics=accuracy,cbs=ModelResetter)\n","learn.fit_one_cycle(10,3e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgW-W5CHnT79"},"source":["This is already better. This is mostly because of the increased precision that was possible because of the freed memory. \n","\n","Now let us do another small tweak which will help us improve our accuracy even better. But first, let us also make our task more challenging. Instead of 3 tokens, we will be using 16 tokens at once into the model, and predicting the next sequence of tokens. This is variable, so you are free to change the sequence length. "]},{"cell_type":"code","metadata":{"id":"TB13MwjNgmNR"},"source":["sl=16 #sequence length\n","seqs=L((tensor(nums[i:i+sl]),tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl))\n","\n","cut=int(len(seqs)*0.8)\n","dls=DataLoaders.from_dsets(group_chunks(seqs[:cut],bs),\n","                           group_chunks(seqs[cut:],bs),\n","                           bs=bs,drop_last=True,shuffle=False) #drop_last=True because you wont be able to ofset the target by one word in that case"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5X867YfmKgs"},"source":["#lets see what this dataset will look like\n","[L(vocab[o] for o in s) for s in seqs[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZfNsxebzol8w"},"source":["You can see that not only is the input sequence of length 16, but the output sequence too is of length 16, which is offset by only one token. This is already a more complicated task."]},{"cell_type":"markdown","metadata":{"id":"0j4eSGwioTM1"},"source":["For this model, we will be modifying the model so that it can process as many tokens as possible. "]},{"cell_type":"code","metadata":{"id":"Q43SKvx5naQ-"},"source":["class LMModel2(Module):\n","    def __init__(self,vocab_sz,n_hidden):\n","        self.i_h=nn.Embedding(vocab_sz,n_hidden) #embedding layer\n","        self.h_h=nn.Linear(n_hidden,n_hidden) #linear layer: to create activations for the next word\n","        self.h_o=nn.Linear(n_hidden,vocab_sz) #final layer to predict the fourth word in the end\n","        self.h=0\n","    \n","    def forward(self,x):\n","        outs=[]\n","        for i in range(sl):\n","            self.h = self.h + self.i_h(x[:,i])\n","            self.h = F.relu(self.h_h(self.h))\n","            outs.append(self.h_o(self.h))\n","        self.h = self.h.detach()\n","        return torch.stack(outs,dim=1) #shape bs,sl,vocab_sz\n","    \n","    def reset(self): self.h = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODml9TXdo12o"},"source":["The output is of shape `(bs,sl,vocab_sz)` while the target is of shape `(bs,sl)`. So we need to change the loss function definition, otherwise it will throw a compatibility issue. All we need to do is to flatten the output before feeding it to the Cross Entropy loss."]},{"cell_type":"code","metadata":{"id":"RjIPQPuPo5Og"},"source":["#we have to modify the loss_func because we stacked the outputs on dimension 1\n","def loss_func(inp,target): return F.cross_entropy(inp.view(-1,len(vocab)),target.view(-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zuHeep0BqIlm"},"source":["And then we can follow the same procedure. Let us also try training for a bit longer and see the results. "]},{"cell_type":"code","metadata":{"id":"ba4V5F91qbRg"},"source":["learn = Learner(dls,LMModel2(len(vocab),64), loss_func=loss_func,metrics=accuracy,cbs=ModelResetter)\n","learn.fit_one_cycle(15,3e-3) #train for longer because its a more complicated task"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMTAViNJqPmZ"},"source":["These are much better results. Now let us do another tweak to our model. What if, instead of a single hidden layer, we stacked two hidden layers, so the output of the second hidden layer is passed as the input to the first? Essentially we're making the model deeper, and it will only help in learning more complex features. "]},{"cell_type":"markdown","metadata":{"id":"nloJTHmBuRz4"},"source":["### Multilayer RNNs\n","\n","Let us save ourselves the trouble of writing this from scratch, and simply use PyTorch's `nn.RNN` class, which can create a sequence of these hidden states. The rest will be the same. "]},{"cell_type":"code","metadata":{"id":"qygBGEH8q9Wm"},"source":["class LMModel3(Module):\n","    def __init__(self,vocab_sz,n_hidden,n_layers):\n","        self.i_h = nn.Embedding(vocab_sz,n_hidden)\n","        self.rnn = nn.RNN(n_hidden,n_hidden,n_layers,batch_first=True) #if we want to create 2 layers stacked together, set n_layers as 2\n","        self.h_o = nn.Linear(n_hidden,vocab_sz)\n","        self.h = torch.zeros(n_layers,bs,n_hidden)\n","\n","    def forward(self,x):\n","        res,h = self.rnn(self.i_h(x),self.h)\n","        self.h = h.detach()\n","        return self.h_o(res)\n","    \n","    def reset(self): self.h.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yxHG044wFHn"},"source":["learn=Learner(dls,LMModel3(len(vocab),64,2), #for now we are using only 2 layers in the RNN\n","              loss_func=CrossEntropyLossFlat(),\n","              metrics=accuracy,cbs=ModelResetter)\n","\n","learn.fit_one_cycle(15,3e-3) #deeper model, exploding/vanishing gradients"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fx7a4DRSrQYS"},"source":["That's weird. Why is this model performing worse than a single layer model? That's because of the exploding/vanishing gradient problem, that arises in deep Neural Networks. \n","\n","Essentially we're multiplying an input by the same matrices again and again. Imagine what happens when you multiply a number again and again by a number. \n","\n","There can be two cases.\n","\n","1. If you're multiplying again and again by a number greater than 1, you will reach an extremely large value soon. And this means, the gradients too will explode, or become very large, in other terms. This may make the model erratic and the performance will deteriorate. \n","\n","2. If you're multiplying again and again by a number less than 0, you will reach an extremely small value soon. This means, the gradients too will be close to zero, and essentially no learning will take place. \n","\n","There are a few methods to prevent these. In the context of Sequence Models, one of the research developments that addressed this issue was the LSTM, or *Long Short Term Memory* Models. . "]},{"cell_type":"markdown","metadata":{"id":"Ov6AUrfiW3lG"},"source":["## Long Short Term Memory (LSTM) Models\n","\n","LSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber and Sepp Hochreiter. In this architecture, there are not one, but two, hidden states. In our base RNN, the hidden state is the output of the RNN at the previous time step. That hidden state is then responsible for two things:\n","\n","* Having the right information for the output layer to predict the correct next\n","token\n","* Retaining memory of everything that happened in the sentence\n","Consider, for example, the sentences “Henry has a dog and he likes his dog very much” and “Sophie has a dog and she likes her dog very much.” It’s very clear that the RNN needs to remember the name at the beginning of the sentence to be able to pre‐ dict he/she or his/her.\n","\n","In practice, RNNs are really bad at retaining memory of what happened much earlier in the sentence, which is the motivation to have another hidden state (called cell state) in the LSTM. The cell state will be responsible for keeping long short-term memory, while the hidden state will focus on the next token to predict. Let’s take a closer look at how this is achieved and build an LSTM from scratch.\n","\n","![](https://drive.google.com/uc?id=1ll89W_Fy0zJN7LNVEqcMvjk7eHmTKk4L)\n","\n","In this picture, our input $x_{t}$ enters on the left with the previous hidden state ($h_{t-1}$) and cell state ($c_{t-1}$). The four orange boxes represent four layers (our neural nets) with the activation being either sigmoid ($\\sigma$) or tanh. tanh is just a sigmoid function rescaled to the range -1 to 1. Its mathematical expression can be written like this:\n","\n","$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} = 2 \\sigma(2x) - 1$$\n","\n","where $\\sigma$ is the sigmoid function. The green circles are elementwise operations. What goes out on the right is the new hidden state ($h_{t}$) and new cell state ($c_{t}$), ready for our next input. The new hidden state is also used as output, which is why the arrow splits to go up.\n","\n","Let's go over the four neural nets (called *gates*) one by one and explain the diagram—but before this, notice how very little the cell state (at the top) is changed. It doesn't even go directly through a neural net! This is exactly why it will carry on a longer-term state.\n","\n","First, the arrows for input and old hidden state are joined together. In the RNN we wrote earlier in this chapter, we were adding them together. In the LSTM, we stack them in one big tensor. This means the dimension of our embeddings (which is the dimension of $x_{t}$) can be different than the dimension of our hidden state. If we call those `n_in` and `n_hid`, the arrow at the bottom is of size `n_in + n_hid`; thus all the neural nets (orange boxes) are linear layers with `n_in + n_hid` inputs and `n_hid` outputs.\n","\n","The first gate (looking from left to right) is called the *forget gate*. Since it’s a linear layer followed by a sigmoid, its output will consist of scalars between 0 and 1. We multiply this result by the cell state to determine which information to keep and which to throw away: values closer to 0 are discarded and values closer to 1 are kept. This gives the LSTM the ability to forget things about its long-term state. For instance, when crossing a period or an `xxbos` token, we would expect to it to (have learned to) reset its cell state.\n","\n","The second gate is called the *input gate*. It works with the third gate (which doesn't really have a name but is sometimes called the *cell gate*) to update the cell state. For instance, we may see a new gender pronoun, in which case we'll need to replace the information about gender that the forget gate removed. Similar to the forget gate, the input gate decides which elements of the cell state to update (values close to 1) or not (values close to 0). The third gate determines what those updated values are, in the range of –1 to 1 (thanks to the tanh function). The result is then added to the cell state.\n","\n","The last gate is the *output gate*. It determines which information from the cell state to use to generate the output. The cell state goes through a tanh before being combined with the sigmoid output from the output gate, and the result is the new hidden state.\n","\n","In terms of code, we can write the same steps like this:\n","\n","(Cited from Deep Learning for Coders with fastai and Pytorch). \n"]},{"cell_type":"code","metadata":{"id":"pZzf8eodwWUG"},"source":["class LSTMCell(Module):\n","    def __init__(self,ni,nh):\n","        self.forget_gate = nn.Linear(ni+nh,nh)\n","        self.input_gate  = nn.Linear(ni+nh,nh)\n","        self.cell_gate   = nn.Linear(ni+nh,nh)\n","        self.output_gate = nn.Linear(ni+nh,nh)\n","    \n","    def forward(self,input,state):\n","        h,c = state\n","        h=torch.stack([h,input],dim=1)\n","        forget = torch.sigmoid(self.input_gate(h)) #value between 0 and 1\n","        c=c*forget\n","        inp=torch.sigmoid(self.input_gate(h))\n","        cell=torch.tanh(self.cell_gate(h))\n","        c=c+ inp*cell\n","        out=torch.sigmoid(self.output_gate(h))\n","        h=out*torch.tanh(c)\n","\n","        return h,(h,c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26GfAgG9ulf9"},"source":["This is almost the same functionality that you will find in PyTorch's `nn.LSTM`. This can easily replace the `nn.RNN` layer in our model. Let's see if that would improve the performance"]},{"cell_type":"code","metadata":{"id":"2B_b5RemX4mX"},"source":["class LMModel4(Module):\n","    def __init__(self,vocab_sz,n_hidden,n_layers):\n","        self.i_h = nn.Embedding(vocab_sz,n_hidden)\n","        self.rnn=nn.LSTM(n_hidden,n_hidden,n_layers,batch_first=True)\n","        self.h_o = nn.Linear(n_hidden,vocab_sz)\n","        self.h = [torch.zeros(n_layers,bs,n_hidden) for _ in range(2)] #LSTM gives 2 outputs, and we store both of them here\n","\n","    def forward(self,x):\n","        res,h = self.rnn(self.i_h(x),self.h)\n","        self.h=[h_.detach() for h_ in h]\n","        return self.h_o(res)\n","\n","    def reset(self): \n","        for h in self.h: h.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXFttMCUb7yu"},"source":["learn=Learner(dls,LMModel4(len(vocab),64,2),\n","              loss_func=CrossEntropyLossFlat(),\n","              metrics=accuracy,cbs=ModelResetter)\n","\n","learn.fit_one_cycle(15,1e-2) #we can train it at a higher learning rate, for a shorter time, and get better accuracy , since it now does not suffer from exploding/vanishing grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h8zmP-Mcn93"},"source":["This is much better! This is solely because LSTM was able to prevent vanishing/exploding gradients to some extent. \n","\n","But its not that LSTMs are completely free from VG/EG problem.\n","We need to perform a few tweaks to this, as was introduced in [this](https://arxiv.org/abs/1708.02182) paper. The resultant LSTM is called the *AWD-LSTM*, which we used to build the text classifier initially. It involves applying certain regularization techniques on the Neural Net layers inside the LSTM, as well as a weight tying method.  \n","\n","### Regularizing LSTMs\n","\n","Some additional things were implemented in this paper as well. However, we will only be implementing Dropout. We will also be implementing weight tying, which is essentially assigning the same weight matrix to the input layer and the output layer.\n","\n","The idea is as follows. The input layers job is to do a mapping from a token to an embedding vector, and the job of the output layer is to do a mapping from an embedding layer to a token. So ideally, they both are doing the same thing, and dont't need two different matrices to perform this task. "]},{"cell_type":"markdown","metadata":{"id":"Klp8TgOgfHp1"},"source":["#### Dropout\n","We have shown a simple demonstration of how to implement dropout in PyTorch, but this is essentially the same as PyTorch's nn.Dropout. (Through PyTorch's native implementation is done in C, not python) "]},{"cell_type":"code","metadata":{"id":"0QqHEx-LcVfS"},"source":["#implementation in PyTorch is really simple. Though PyTorch's native layer is written in C, not python\n","class Dropout(Module):\n","    def __init__(self,p): self.p=p\n","\n","    def forward(self,x):\n","        if not self.training: return x #pytorch retains the state of the optimizer (whether we are in training process, or validation process). Dropout is one of the main reasons for this\n","        mask=x.new(*x.shape).bernoulli_(1-p) #retain 1-p neurons randomly. This random process is called the bernoulles process of selection\n","        return x*mask.div_(1-p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUMm4un-f-QP"},"source":["??nn.Dropout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjRHBH_ngB1b"},"source":["??F.dropout\n","#torch._VF or torch. variable functions module is an alias of torch._C._VariableFunctions\n","# https://github.com/pytorch/pytorch/blob/master/torch/_VF.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xasi0OTOzcpP"},"source":["And integrating this into our model is as simple as follows"]},{"cell_type":"code","metadata":{"id":"TbTqh1jfgCoz"},"source":["class LMModel5(Module):\n","    def __init__(self,vocab_sz,n_hidden,n_layers,p):\n","        self.i_h = nn.Embedding(vocab_sz,n_hidden)\n","        self.rnn = nn.LSTM(n_hidden,n_hidden,n_layers,batch_first=True)\n","        self.drop=nn.Dropout(p)\n","        self.h_o=nn.Linear(n_hidden,vocab_sz)\n","        self.h_o.weight=self.i_h.weight #weight tying\n","        self.h=[torch.zeros(n_layers,bs,n_hidden) for _ in range(2)]\n","\n","    def forward(self,x):\n","        raw,h = self.rnn(self.i_h(x),self.h)\n","        out=self.drop(raw)\n","        self.h = [h_.detach() for h_ in h]\n","        return self.h_o(out),raw,out #we've to return 3 things\n","    \n","    def reset(self):\n","        for h in self.h: h.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgyCZe6G0s5S"},"source":["Let us build our model using this architecture"]},{"cell_type":"code","metadata":{"id":"IWCRvZSRkz0L"},"source":["learn=TextLearner(dls,LMModel5(len(vocab),64,2,0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dylPRZ4lJW9"},"source":["learn.fit_one_cycle(15,1e-2,wd=0.1) #adding some additional regularization"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fLKl1Dk0xa0"},"source":["This is amazing. The performance has vastly improved. With this we finish our discussion on LSTMs. Now let us move to the present state-of-the-art architecture used in NLP. This architecture is called as `Transformer`, which originated from [this](https://arxiv.org/abs/1706.03762) paper. We won't be explaining each module step by step, but if you are interested, you can find an intuitive explanation [here](https://www.youtube.com/watch?v=4Bdc55j80l8). \n","\n","You may have heard about the famous GPT models from OpenAI, which have performed some really complex tasks, like talking, writing code, building websites from mere vague ideas given as inputs, create art, and some other really amazing things. You can find a brief demo [here](https://www.youtube.com/watch?v=PqbB07n_uQ4), in which a person interviews an AI model, and the results are really amazing. I recommend you watch it fully. "]},{"cell_type":"markdown","metadata":{"id":"KD2eJWzEm5ro"},"source":["## Transformers\n","\n","We'll essentially be using pretrained hodels that are provided by HuggingFace's transformer library. You can find the source code [here](https://github.com/huggingface/transformers) and the documentations [here](https://huggingface.co/transformers/pretrained_models.html). Let us start by building our models and finetuning transformers for our specific tasks. "]},{"cell_type":"code","metadata":{"id":"VGJtfQqtloO2"},"source":["!pip install -Uq transformers >./temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9omfMNJRq7vw"},"source":["from transformers import GPT2LMHeadModel,GPT2TokenizerFast"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5x9IJbS12jur"},"source":["So we'll be using a basic version of Transformers, called as GPT2, which itself is quite memory intensive. However, you can explore by changing the model by looking up the documentations. "]},{"cell_type":"code","metadata":{"id":"YBpFC2wSrC_T"},"source":["pretrained_weights='gpt2' #basic version, which itself takes a lot of memory\n","tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n","model=GPT2LMHeadModel.from_pretrained(pretrained_weights) #this is a pretrained model."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRW4yqqS2wF4"},"source":["Just to get a brief idea about what our model looks like, let us look into the representation of the model"]},{"cell_type":"code","metadata":{"id":"aUrAOC8Q13Sy"},"source":["model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mkHOhQBE25Rd"},"source":["As we saw before, we need to take a peice of text, tokenize it, and feed it to the model. the transformers library already provides us with a vicabulary that was used to train the model, so let us see what the model performs just without any finetuning. "]},{"cell_type":"code","metadata":{"id":"DRf8IPvFr_WP"},"source":["#lets see how the tokenizer works\n","ids=tokenizer.encode('Welcome to the 13th session on Practical Machine Learning')\n","ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DYw_H7SsKj0"},"source":["#you can even decode the tokens back to original words\n","tokenizer.decode(ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4yHXP66sY-9"},"source":["#since our model is pretrained, you can directly use it to get predictions\n","t=torch.LongTensor(ids)[None]\n","preds=model.generate(t,max_length=50) #by default, the length of predictions is 20 (words). You can change it using the max_length parameter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8qb5WqFs1MA"},"source":["preds.shape,preds[0] #by default, the predictions are of length 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w1FLB9PX3O8n"},"source":["The output is in the form of numerical tokens. Let us decode these into words, to see what the model has predicted. "]},{"cell_type":"code","metadata":{"id":"K2JzwIzms6J_"},"source":["tokenizer.decode(preds[0].numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LsvDdKGO4DGQ"},"source":["This is pretty impressive on its own. But let us fine tune this model onn our own dataset. We'll be using a smaller version of WIKITEXT, called WIKITEXT2. (Just to speed up the training process)"]},{"cell_type":"markdown","metadata":{"id":"kcLHLdMStdpf"},"source":["### Finetuning the Transformers Model\n","\n","Let us first download the dataset and set it up. Our task it to build  a language model that takes in a series of text, and also outputs the same series, just offset by one token."]},{"cell_type":"code","metadata":{"id":"V6mAdIwms88K"},"source":["path = untar_data(URLs.WIKITEXT_TINY)\n","path.ls()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDgsRrQOtkXH"},"source":["df_train=pd.read_csv(path/'train.csv',header=None)\n","df_valid=pd.read_csv(path/'test.csv',header=None)\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7R2qzpCi4sWO"},"source":["As we mentioned before, when building Language Models, we don't care what data is training set, and what is validation set. So we simply club all the data together. "]},{"cell_type":"code","metadata":{"id":"tGmbmDO1tymV"},"source":["#concatenating all texts in one array\n","all_texts = np.concatenate([df_train[0].values,df_valid[0].values])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtjpGyMmumEC"},"source":["Now let us build a transformation function that takes in words, and converts them into numericalized tokens. Its just a more efficient way of doing this. Previously, we converted all text to numbers beforehand. Now we will do this while running the model. \n","\n","In a fastai Transform you can define:\n","\n","* an encodes method that is applied when you call the transform (a bit like the forward method in a nn.Module)\n","\n","* a decodes method that is applied when you call the decode method of the transform, if you need to decode anything for showing purposes (like converting ids to a text here)\n","\n","* a setups method that sets some inner state of the Transform (not needed here so we skip it)"]},{"cell_type":"code","metadata":{"id":"hDL-oJonumec"},"source":["class TransformersTokenizer(Transform):\n","    def __init__(self,tokenizer): self.tokenizer=tokenizer\n","    def encodes(self,x):\n","        toks=self.tokenizer.tokenize(x)\n","        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n","    def decodes(self,x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaB9HuADv0F9"},"source":["splits=[range_of(df_train),list(range(len(df_train),len(all_texts)))]\n","tls=TfmdLists(all_texts,TransformersTokenizer(tokenizer),splits=splits,dl_type=LMDataLoader) #LMDataloader, because this is a LM problem, dependent var itself being a sequence\n","\n","#ignore this error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aHNGTE7xsmB"},"source":["tls.train[0],tls.valid[0] #look the same, but only begin and end the same way"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TBfOkrMyCi9"},"source":["tls.tfms(tls.train.items[0]).shape, tls.tfms(tls.valid.items[0]).shape #see? they're not exaclty the same. They infact have different shapes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_If6eoME5OTk"},"source":["Let us see how we can use the Transform's decode function to decode the numericalised tokens back to words, using the `show_at` function in the fastai library. "]},{"cell_type":"code","metadata":{"id":"Wbe1gg7SyVdi"},"source":["#we can have a look at decodes using the show_at function\n","show_at(tls.train,0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yjxd7Ukk5bWU"},"source":["Now that our dataset is setup, let us put the data into dataloaders."]},{"cell_type":"code","metadata":{"id":"45VVtAJjzEA-"},"source":["bs,sl = 4,256\n","dls = tls.dataloaders(bs=bs,seq_len=sl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cF_CDcVQzXds"},"source":["dls.show_batch(max_n=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GaJpfs7i08VM"},"source":["There is one small tweak that needs to be done. \n","\n","The HuggingFace model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them in some regularization scheme). To work inside the fastai training loop, we will need to drop those using a Callback: we use those to alter the behavior of the training loop.\n","\n","Here we need to write the event `after_pred` and replace `self.learn.pred` (which contains the predictions that will be passed to the loss function) by just its first element. In callbacks, there is a shortcut that lets you access any of the underlying Learner attributes so we can write `self.pred[0]` instead of `self.learn.pred[0]`. That shortcut only works for read access, not write, so we have to write `self.learn.pred` on the right side (otherwise we would set a pred attribute in the Callback)."]},{"cell_type":"code","metadata":{"id":"DdHsonTqzbVS"},"source":["class DropOutput(Callback):\n","    def after_pred(self): self.learn.pred = self.pred[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0wlYEba5sse"},"source":["Let us build our learner class."]},{"cell_type":"code","metadata":{"id":"GwN2xJ4k2lDU"},"source":["learn = Learner(dls,model,loss_func=CrossEntropyLossFlat(),cbs=[DropOutput,RNNRegularizer],metrics=Perplexity()).to_fp16() \n","#not using Accuracy because its a bad metric for RNNs. Even for very good models, the acuracy will reemain only 30% or so"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSj-rrrO5uo_"},"source":["Actually, before training, let us see how the model is working as is. "]},{"cell_type":"code","metadata":{"id":"QZnQ9vsD51ZD"},"source":["learn.validate()\n","#seeing model performance even without training. Its actually good. In the vanilla RNN, the perplexity was around 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TfsItHaj52RO"},"source":["This is pretty good. The perplexity is much lower than the value we saw in LSTM models. And we havent even trained yet!"]},{"cell_type":"markdown","metadata":{"id":"FD2MbJpF59Fs"},"source":["Let us train our model for one epoch, and see how it performs. "]},{"cell_type":"code","metadata":{"id":"TXkZL9_J2stY"},"source":["learn.fit_one_cycle(1, 1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LGedsUjU6CJ8"},"source":["The perplexity has improved quite a lot. Let us see how the model performs now. "]},{"cell_type":"code","metadata":{"id":"YJmnNp4t3ECh"},"source":["prompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\"\n","prompt_ids = tokenizer.encode(prompt)\n","inp = tensor(prompt_ids)[None].cuda()\n","inp.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmUknne55j1L"},"source":["preds=learn.model.generate(inp,max_length=50,num_beams=5,temperature=1.5)\n","tokenizer.decode(preds[0].cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNbDnxLm6RI3"},"source":["## Review\n","\n","In this session, we learnt about NLP in Deep Learning. We learnt how to train state of the art models in NLP, including Vanilla RNNs, LSTMs, and Transformers. We learnt about self-supervised learning techniques in NLP as well. Hopefully this was an informative session. "]},{"cell_type":"markdown","metadata":{"id":"4-DrZmri6qK0"},"source":["## Exercise \n","Your task is to train the transformer model on the text classification task using the ULMFiT Approach. You can use the word tokenization approach. The Transformer model is not exactly trained on WIKITEXT data, but you can use model as is, as it has already been pretrained. So you need to finetune it on the IMDb dataset, followed by the text classification process. "]},{"cell_type":"code","metadata":{"id":"gwwf9fZoOwrI"},"source":["path=untar_data(URLs.IMDB)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXpKfpYD5wA6"},"source":["dls_lm = DataBlock(\n","    blocks=TextBlock.from_folder(path,is_lm=True), #is_lm tells that explicit labels are not required, and the target would simply be text offset by one token\n","    get_items=get_imdb, splitter = RandomSplitter(0.1)\n",").dataloaders(path,path=path,bs=128,seq_len=80) #each batch would be of length 80 tokens, and a total of 128 batches are required"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i168W2FEPHHH"},"source":["pretrained_weights='gpt2' #basic version, which itself takes a lot of memory\n","tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n","model=GPT2LMHeadModel.from_pretrained(pretrained_weights) #this is a pretrained model."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vv3MP9XAO4QU"},"source":["learn=language_model_learner(\n","    dls_lm,arch=model,drop_mult=0.3,\n","    metrics=[accuracy,Perplexity()]).to_fp16()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpO-ZR7GPIj0"},"source":["learn.fit_one_cycle(1,2e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cidDu4pxPXKK"},"source":["learn.save_encoder(path_mdl/'finetuned_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"om7FfoIQPR-A"},"source":["Classification"]},{"cell_type":"code","metadata":{"id":"dgsgFPZjPRmg"},"source":["dls_classification = DataBlock(\n","    blocks=(TextBlock.from_folder(path,vocab=dls_lm.vocab),CategoryBlock), #we don't pass is_lm=True to TextBlock.from_folder\n","    #we also pass in the vocab here, because otherwise the vocab generated here may not match the vocab used in the fintuning task, which may lead to results which makes no sense\n","    get_y=parent_label,\n","    get_items=partial(get_text_files,folders=['train','test']), #now we dont used the unsupervised text anymore\n","    splitter=GrandparentSplitter(valid_name='test') \n",").dataloaders(path,path=path,bs=128,seq_len=72)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gg8ibbNsPigf"},"source":["learn=text_classifier_learner(dls_classification,model,drop_mult=0.5, metrics=accuracy).to_fp16()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GmW5KJVRPnwt"},"source":["learn = learn.load_encoder(path_mdl/'finetuned_encoder')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0y-TOsc6Poc1"},"source":["learn.fit_one_cycle(1,2e-2)"],"execution_count":null,"outputs":[]}]}